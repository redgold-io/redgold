# Addendum

## Treasury Management & Reserves

The treasury is a core component of the network. It's responsible for managing the reserves of the network, 
and is managed with a decentralized DAO built around the active seeds. It's important to have a treasury smoothing 
function on the AMM for the native token, so that the network can manage its own reserves and deal with volatility.

## Issues with Proof of Stake

Another huge issue with proof of stake systems is that they’re not really providing a replacement for 
conventional legal liability, as the suffering party has no recourse against a majority decision.

In a conventional trust, if you give you make a contract with someone and 
they violate that contract (equivalent of double spend issue, i.e. someone reverses a transaction,) 
the merchant has lost $100. Ethereum stake does not actually cover this situation AT ALL because it’s
not a real guarantee. The merchant has been told the network actually ‘accepted’ something but it later 
reversed the decision — the stake doesn’t go towards covering the cost entirely, as the network does not 
provide any guarantees.

Really, bitcoin doesn't solve this problem much either -- because it's fundamentally reversible under 
certain conditions. 

## Decentralized KYC 

Many attempts at on-chain KYC exist, all should be integrated with and used, custodial pools should be distinguished 
as those which require KYC and those that do not, with the decision whether or not to require 

## Approach towards smart contracts

Smart contracts should be viewed as a plug-and-play ecosystem component. The desired languages for writing trading 
strategies or other more sophisticated models are things like python. This means the executors should be isolated 
from the contracts themselves. The same model for deposit and ETF products can be applied to arbitrary code 
execution. There is a requirement that the code be audited and accepted into the network in order for this model 
to function. So it does not act as a 'universal computer' ala Ethereum, but so long as there exist enough audited 
contract code a user should be able to easily build their desired behavior. Contracts via deposits can then make 
use of an underlying asset, which most crypto contracts prevent by requiring you to remain within the network asset.

This approach allows us to plug-in arbitrary connections to other integration platforms and smart contract platforms, 
so that we may re-use the functionality others are building.


## Why not just use blocks?

Even if you have blocks, you're still assuming that someone will store them. 
Eventual consistency, the only use of blocks is for partitioning large data sets
No proof of work means there's no issue with forging blocks
You still are relying on an authority for determining whats the best block. 
Small PoW networks get hacked easily.

It may be convenient to use blocks for synchronization etc. but these are all easily solvable 
problems with other strategies. the core prioritization of the network should focus EXCLUSIVELY on 
disagreements. Most of the time for most operations, you'll never really need to deal with that.

## Implementation of Trading Strategies.

this allows you to build a trading strategy and launch it as an ETF. 
for beginning of network, single class path, approved ETFs and code 
long term, integration with other contract providers (any other DSLs for apps)
built around the deposit chain. 

## State Machines 
"A blockchain runtime is a state machine. It has some internal state, and state transition function 
that allows it to transition from its current state to a future state. In most runtimes there
are states that have valid transitions to multiple future states, but a single transition must be selected."

This isn't true, it doesn't have to be represented as a state machine. 
The fundamental architecture is only dependencies. 
One transaction depends on other transactions that were previously approved. 
No state is required in this. Only agreement on which additions to the graph 
are valid.

State transitions imply a unified state function. 
Instead the state is localized relative to the dependencies.

The future of many small coins demands ETF management style approaches. 
The coins will be too low liquidity to demand anything other than active market making.
+ passive indexing.


New nodes end up trusting prior nodes to validate old data anyways. Think of when joining the network.


Start with a model that is ->

Bias towards central nodes based on history (white listing essentially — minor bias)
Form quasi governance council based on network history — 
this adds another layer of protection — Cap rewards to prevent abuse —
require contributions etc.

Governance council + central bias act is the averaging layer, from which influence metrics + DATT scores are calculated.

Use eigentrust temporarily

All traffic and routing follows priority patterns setup by trust model.

Deposit model is similar, where there needs to be a partitioning such that there's
different groups of nodes protecting each sub-partition.
The node groups form overlapping, with the overlap constant higher towards 
network centrality bias.

## Why Rust?

P2P protocols shouldn't really belong exclusively to one language. Libp2p has demonstrated the value of building 
implementations in many languages, and 
Should be using any and all languages that support the protocol
Long term goal is to actually subdivide all the responsibilities of this into proper separate
decentralized services. I.e. a service that provides a database layer that's audited.
That's highly scalable.

## Unfinished Notes TBD

Its also functionally equivalent so long as one node maintains the same database entry that it's operating on, 
you can still preserve security and locality. 



Something that hasn't really been touched on is the origin of contracts and currency itself. 

Currency really starts from a single, or multi-party 'seed' that grows in usage as people subscribe to it. 
Really ANYONE should be able to do this, within a set of agreed upon contract standards or software. 

Any 'proper' model will attempt to capture as much of this information as possible. I.e. if there's a software 
fork or disagreement in code execution or something else, there should be a peaceful way to merge these two things 
together somehow. I.e. think of it like 2 different classloaders, if we simply generalize across them then there's 
still a protocol that functions across both classloaders. That's simply a different execution environment. 

Really security is the main thing by offered by all this kinda stuff. Real applications only need integration points.


This helps fight MEV problems, because validators simply accept whatever fees they’re incentivized to accept.

Transaction safety, revert to previous address if unclaimed — with time code for revert to fee address.

basic blurb about the executor model. One set of classes we can load against
for multi-tenant executors.

Another for supporting binary executors on WASM reading from stdin/stdout

Another for dealing with external libraries.

All data should be fully validatable, what keeps it up to date is guarantees from
fees. As soon as the data goes out of scope and can no longer be validated, all
values from it are RECONVERTED backed to 'FIXED', in other words an action
is evaluated with a RECLAIM transaction which goes to rewards pool.

Inputs have 'floating dependencies' as lazily evaluated arguments -- but
fees must pay for intermediary data storage -- OR else they get evaluated.
Every preparation of a transaction must execute all the actions for a fee estimate.
You must pay for a fee estimate from a reputable node as well.
And use a collateral argument.

reactive subsets of a transaction dependency tree


section on schema decisions. 
 

## Useful PoW 

Transitioning away from wasteful PoW makes sense for many reasons, but the core principle of it offering an 
expensive protection against attacks does make sense, it just doesn't provide anything useful. Instead, the key 
model for capturing this effect can be accomplished by performing some valuable work. The main problem with this, 
of course, is that it can be faked or repeated by a copycat attack -- if the work is independent of the active chain. 

The only way then to capture some 'useful' work, is to restrict it to a task that can depend on a hash definition, 
without materializing changing either the verification or output -- AND ideally which can be verified more quickly than 
it can be calculated. An example of a simple 'useful' proof of work might be to take a recent hash reference (of 
your own nodes observation chain,) and supply that as a seed in a process which requires randomness. For instance 
instantiating an ML model with parameters randomly seeded from that hash and training it. While a 'true' verification 
would require completely re-building the model and cost the same as the output, a partial verification would be to 
use samples from a hidden hold-out and verify the accuracy is consistent.

Producing these models, would then provide a 'useful' proof of work with a low verification cost. This model does 
have some flaws though, as the hold-out set would have to remain secret in order to act as a useful verification 
mechanism, and it would have to be specific to the model of interest.

## Cross-Embeddings